{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063e6079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29ddbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\notebooks\\venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4450ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (3.12.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (6.5.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->datasets) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: colorama in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a0a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 01:42:45,749 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\logs\n",
      "2025-06-27 01:42:45,751 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\processed\n",
      "2025-06-27 01:42:45,752 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\eval\n",
      "2025-06-27 01:42:45,753 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\results\n",
      "2025-06-27 01:42:45,755 - INFO - Starting prediction process at 2025-06-27 01:42:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR      : d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\n",
      "PATH_CSV      : d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\processed\\cases.csv\n",
      "PATH_QUERIES  : d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\eval\\queries.json\n",
      "PATH_RESULTS  : d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\results\n",
      "LOG_PATH      : d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\logs\\prediction.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 01:42:45,955 - INFO - Loaded 67 cases from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\processed\\cases.csv and 43 queries from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\eval\\queries.json\n",
      "2025-06-27 01:42:45,972 - INFO - TF-IDF matrix shape: (67, 2000)\n",
      "2025-06-27 01:42:46,316 - INFO - Training data shape: (430, 4003)\n",
      "2025-06-27 01:42:46,317 - INFO - Class distribution: [215 215]\n",
      "2025-06-27 01:42:46,642 - INFO - Best Logistic Regression Parameters: {'C': 0.1}\n",
      "2025-06-27 01:42:46,645 - INFO - Best CV F1 Score (LogReg): 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Parameters: {'C': 0.1}\n",
      "Best CV F1 Score (LogReg): 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 01:42:46,940 - INFO - Best SVM Parameters: {'C': 1}\n",
      "2025-06-27 01:42:46,942 - INFO - Best CV F1 Score (SVM): 1.00\n",
      "2025-06-27 01:42:46,943 - INFO - Loading IndoBERT model on cpu\n",
      "2025-06-27 01:42:46,948 - INFO - Load pretrained SentenceTransformer: indobenchmark/indobert-base-p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Parameters: {'C': 1}\n",
      "Best CV F1 Score (SVM): 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 01:42:47,262 - WARNING - No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n",
      "2025-06-27 01:43:00,179 - INFO - Document embeddings shape: (67, 768)\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.37it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.43it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.68it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.29it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.45it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.47it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.85it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.92it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.55it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.66it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.88it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.68it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.59it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.06it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.27it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.09it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.29it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.03it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.05it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.08it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.50it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.81it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.47it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.07it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.03it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.40it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.64it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.89it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.66it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.80it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.78it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.78it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.55it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.04it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.83it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.96it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Predicting: 100%|██████████| 67/67 [01:06<00:00,  1.01it/s]\n",
      "2025-06-27 01:44:06,400 - INFO - Saved results to d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\results/logreg_predictions.csv\n",
      "2025-06-27 01:44:06,401 - INFO - Saved results to d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\results/svm_predictions.csv\n",
      "2025-06-27 01:44:06,403 - INFO - Saved results to d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\results/indobert_predictions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\results/logreg_predictions.csv\n",
      "✅ Saved to d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\results/svm_predictions.csv\n",
      "✅ Saved to d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\data\\results/indobert_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Import Libraries and Initialize\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    # ✅ Jika dijalankan dari file .py\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # ✅ Jika dijalankan dari Jupyter atau IPython\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "# 📁 Path lainnya tetap sama\n",
    "PATH_CSV = os.path.join(BASE_DIR, 'data', 'processed', 'cases.csv')\n",
    "PATH_QUERIES = os.path.join(BASE_DIR, 'data', 'eval', 'queries.json')\n",
    "PATH_RESULTS = os.path.join(BASE_DIR, 'data', 'results')\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "LOG_PATH = os.path.join(LOG_DIR, 'prediction.log')\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Debug print untuk memastikan semua path sudah tepat\n",
    "print(f\"BASE_DIR      : {BASE_DIR}\")\n",
    "print(f\"PATH_CSV      : {PATH_CSV}\")\n",
    "print(f\"PATH_QUERIES  : {PATH_QUERIES}\")\n",
    "print(f\"PATH_RESULTS  : {PATH_RESULTS}\")\n",
    "print(f\"LOG_PATH      : {LOG_PATH}\")\n",
    "\n",
    "\n",
    "# Validate path length for Windows\n",
    "MAX_PATH_LENGTH = 260\n",
    "\n",
    "def validate_path(path):\n",
    "    if len(path) > MAX_PATH_LENGTH:\n",
    "        raise ValueError(f\"Path {path} exceeds Windows maximum length of {MAX_PATH_LENGTH} characters\")\n",
    "    return path\n",
    "\n",
    "# Ensure directories exist\n",
    "for path in [LOG_DIR, os.path.dirname(PATH_CSV), os.path.dirname(PATH_QUERIES), PATH_RESULTS]:\n",
    "    try:\n",
    "        validate_path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logging.info(f\"Directory ensured: {path}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Path validation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH, mode='a', encoding='utf-8'),  # Append mode\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Starting prediction process at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Part 2: Setup Environment\n",
    "def setup_environment():\n",
    "    \"\"\"Ensure NLTK resources are available.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        logger.info(\"Downloading NLTK punkt tokenizer\")\n",
    "        nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Part 3: Preprocess Text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text: lowercase, remove specific legal terms, tokenize, normalize spaces.\"\"\"\n",
    "    try:\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'\\b(?:putusan|nomor_perkara|tahun|pengadilan|hakim)\\b', '', text)\n",
    "        text = re.sub(r'uu\\s+no', 'undang_undang_nomor', text)\n",
    "        text = re.sub(r'pasal\\s+\\d+', 'pasal', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        tokens = word_tokenize(text)\n",
    "        return ' '.join(tokens) if tokens else 'empty'\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing text: {e}\")\n",
    "        return 'empty'\n",
    "\n",
    "# Part 4: Load Data\n",
    "def load_data():\n",
    "    \"\"\"Load cases.csv and queries.json.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(PATH_CSV)\n",
    "        texts = df['ringkasan_fakta'].fillna('').apply(preprocess_text).tolist()\n",
    "        case_ids = df['case_id'].tolist()\n",
    "        if not texts or not case_ids:\n",
    "            raise ValueError(\"Dataset is empty or missing required columns ('ringkasan_fakta', 'case_id').\")\n",
    "        with open(PATH_QUERIES, 'r', encoding='utf-8') as f:\n",
    "            queries = json.load(f)\n",
    "        case_solutions = {item['case_id']: item.get('solution', '') for item in queries}\n",
    "        logger.info(f\"Loaded {len(df)} cases from {PATH_CSV} and {len(queries)} queries from {PATH_QUERIES}\")\n",
    "        return df, texts, case_ids, queries, case_solutions\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found: {e}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Part 5: Setup TF-IDF\n",
    "def setup_tfidf(texts: List[str]) -> tuple:\n",
    "    \"\"\"Initialize TF-IDF vectorizer with reduced features.\"\"\"\n",
    "    stop_words = [\n",
    "        'dan', 'di', 'dari', 'ke', 'pada', 'dengan', 'untuk', 'yang', 'ini', 'itu',\n",
    "        'adalah', 'tersebut', 'sebagai', 'oleh', 'atau', 'tetapi', 'karena', 'jika',\n",
    "        'dalam', 'bagi', 'tentang', 'melalui', 'serta', 'maka', 'lagi', 'sudah',\n",
    "        'belum', 'hanya', 'saja', 'bahwa', 'apa', 'siapa', 'bagaimana', 'kapan',\n",
    "        'dimana', 'kenapa', 'sejak', 'hingga', 'agar', 'supaya', 'meskipun', 'walau',\n",
    "        'kecuali', 'terhadap', 'antara', 'selain', 'setiap', 'sebelum', 'sesudah'\n",
    "    ]\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words=stop_words)\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        logger.info(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "        return vectorizer, tfidf_matrix\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up TF-IDF: {e}\")\n",
    "        print(f\"Error setting up TF-IDF: {e}\")\n",
    "        raise\n",
    "\n",
    "# Part 6: Extract Features\n",
    "def extract_features(query_vec, doc_vec, query_text: str, doc_text: str) -> np.ndarray:\n",
    "    \"\"\"Extract features for Logistic Regression and SVM.\"\"\"\n",
    "    try:\n",
    "        query_vec = query_vec.toarray()[0]\n",
    "        doc_vec = doc_vec.toarray()[0]\n",
    "        cos_sim = cosine_similarity([query_vec], [doc_vec])[0][0]\n",
    "        query_words = set(query_text.split())\n",
    "        doc_words = set(doc_text.split())\n",
    "        overlap = len(query_words.intersection(doc_words)) / max(len(query_words), 1)\n",
    "        coverage = overlap\n",
    "        return np.concatenate([query_vec, doc_vec, [cos_sim, overlap, coverage]])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting features: {e}\")\n",
    "        return np.zeros(4003)  # 2000 + 2000 + 3\n",
    "\n",
    "# Part 7: Prepare Training Data\n",
    "def prepare_training_data(queries, case_ids, texts, vectorizer, tfidf_matrix):\n",
    "    \"\"\"Prepare training data with balanced sampling.\"\"\"\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for item in queries:\n",
    "        query = preprocess_text(item['query'])\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        true_id = item['case_id']\n",
    "        try:\n",
    "            true_idx = case_ids.index(true_id)\n",
    "        except ValueError:\n",
    "            logger.warning(f\"Case ID {true_id} not found in dataset. Skipping query: {item['query'][:50]}...\")\n",
    "            continue\n",
    "        true_vec = tfidf_matrix[true_idx]\n",
    "        pos_features = extract_features(query_vec, true_vec, query, texts[true_idx])\n",
    "        neg_indices = [i for i in range(len(case_ids)) if i != true_idx]\n",
    "        neg_samples = np.random.choice(neg_indices, size=min(5, len(neg_indices)), replace=False)\n",
    "        for neg_idx in neg_samples:\n",
    "            neg_vec = tfidf_matrix[neg_idx]\n",
    "            neg_features = extract_features(query_vec, neg_vec, query, texts[neg_idx])\n",
    "            X_train.append(pos_features - neg_features)\n",
    "            y_train.append(1)\n",
    "            X_train.append(neg_features - pos_features)\n",
    "            y_train.append(0)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    logger.info(f\"Training data shape: {X_train.shape}\")\n",
    "    logger.info(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "    return X_train, y_train\n",
    "\n",
    "# Part 8: Train Models\n",
    "def train_models(X_train, y_train):\n",
    "    \"\"\"Train Logistic Regression and SVM with stratified cross-validation.\"\"\"\n",
    "    try:\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        param_grid = {'C': [0.1, 1, 10]}\n",
    "        logreg = GridSearchCV(\n",
    "            LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear'),\n",
    "            param_grid, cv=skf, scoring='f1', n_jobs=-1\n",
    "        )\n",
    "        logreg.fit(X_train, y_train)\n",
    "        logger.info(f\"Best Logistic Regression Parameters: {logreg.best_params_}\")\n",
    "        logger.info(f\"Best CV F1 Score (LogReg): {logreg.best_score_:.2f}\")\n",
    "        print(f\"Best Logistic Regression Parameters: {logreg.best_params_}\")\n",
    "        print(f\"Best CV F1 Score (LogReg): {logreg.best_score_:.2f}\")\n",
    "        svm = GridSearchCV(\n",
    "            LinearSVC(max_iter=1000, class_weight='balanced', tol=1e-3),\n",
    "            param_grid, cv=skf, scoring='f1', n_jobs=-1\n",
    "        )\n",
    "        svm.fit(X_train, y_train)\n",
    "        logger.info(f\"Best SVM Parameters: {svm.best_params_}\")\n",
    "        logger.info(f\"Best CV F1 Score (SVM): {svm.best_score_:.2f}\")\n",
    "        print(f\"Best SVM Parameters: {svm.best_params_}\")\n",
    "        print(f\"Best CV F1 Score (SVM): {svm.best_score_:.2f}\")\n",
    "        return logreg, svm\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training models: {e}\")\n",
    "        print(f\"Error training models: {e}\")\n",
    "        raise\n",
    "\n",
    "# Part 9: Setup IndoBERT and BM25\n",
    "def setup_indobert_and_bm25(texts: List[str]):\n",
    "    \"\"\"Initialize IndoBERT and BM25 models.\"\"\"\n",
    "    try:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger.info(f\"Loading IndoBERT model on {device}\")\n",
    "        bi_encoder = SentenceTransformer('indobenchmark/indobert-base-p1', device=device)\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device, max_length=512)\n",
    "        doc_embeddings = bi_encoder.encode(\n",
    "            texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False, batch_size=16\n",
    "        )\n",
    "        bm25 = BM25Okapi([t.split() for t in texts])\n",
    "        logger.info(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
    "        return bi_encoder, cross_encoder, doc_embeddings, bm25\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up Indo-BERT/BM25: {e}\")\n",
    "        print(f\"Error setting up Indo-BERT/BM25: {e}\")\n",
    "        raise\n",
    "\n",
    "# Part 10: Retrieval Functions\n",
    "def logreg_retrieve(query: str, vectorizer, tfidf_matrix, case_ids, texts, logreg, k: int = 5) -> List[str]:\n",
    "    \"\"\"Retrieve top-k cases using Logistic Regression.\"\"\"\n",
    "    try:\n",
    "        query = preprocess_text(query)\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        scores = []\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            doc_vec = tfidf_matrix[i]\n",
    "            features = extract_features(query_vec, doc_vec, query, texts[i])\n",
    "            score = logreg.predict_proba([features])[0][1]  # Probability of positive class\n",
    "            scores.append((case_ids[i], score))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [x[0] for x in scores[:k]]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in LogReg retrieval for query '{query[:50]}...': {e}\")\n",
    "        return []\n",
    "\n",
    "def svm_retrieve(query: str, vectorizer, tfidf_matrix, case_ids, texts, svm, k: int = 5) -> List[str]:\n",
    "    \"\"\"Retrieve top-k cases using SVM.\"\"\"\n",
    "    try:\n",
    "        query = preprocess_text(query)\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        scores = []\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            doc_vec = tfidf_matrix[i]\n",
    "            features = extract_features(query_vec, doc_vec, query, texts[i])\n",
    "            score = svm.decision_function([features])[0]\n",
    "            scores.append((case_ids[i], score))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [x[0] for x in scores[:k]]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in SVM retrieval for query '{query[:50]}...': {e}\")\n",
    "        return []\n",
    "\n",
    "def indobert_retrieve(query: str, bi_encoder, cross_encoder, doc_embeddings, bm25, case_ids, texts, k: int = 10, alpha: float = 0.7) -> List[str]:\n",
    "    \"\"\"Retrieve top-k cases using IndoBERT with BM25 hybrid scoring.\"\"\"\n",
    "    try:\n",
    "        query = preprocess_text(query)\n",
    "        query_vec = bi_encoder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "        sim_scores = cosine_similarity([query_vec], doc_embeddings)[0]\n",
    "        bm25_scores = bm25.get_scores(query.split())\n",
    "        bm25_scores /= np.max(bm25_scores) + 1e-10\n",
    "        combined = alpha * sim_scores + (1 - alpha) * bm25_scores\n",
    "        top_k_idx = np.argsort(combined)[-k:][::-1]\n",
    "        rerank_pairs = [[query, texts[i]] for i in top_k_idx]\n",
    "        rerank_scores = cross_encoder.predict(rerank_pairs)\n",
    "        reranked_idx = np.argsort(rerank_scores)[::-1][:min(5, len(rerank_scores))]\n",
    "        return [case_ids[top_k_idx[i]] for i in reranked_idx]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in IndoBERT retrieval for query '{query[:50]}...': {e}\")\n",
    "        return []\n",
    "\n",
    "# Part 11: Predict Outcome\n",
    "def predict_outcome(query: str, retrieve_fn, case_solutions, true_solution=None):\n",
    "    \"\"\"Predict outcome based on retrieved cases.\"\"\"\n",
    "    try:\n",
    "        top_5_ids = retrieve_fn(query)\n",
    "        solutions = [case_solutions.get(cid, '') for cid in top_5_ids]\n",
    "        filtered = [s for s in solutions if s and s not in ['nan', None]]\n",
    "        predicted = max(set(filtered), key=filtered.count) if filtered else 'Tidak ditemukan'\n",
    "        metrics = {}\n",
    "        if true_solution and filtered:\n",
    "            y_true = [1 if true_solution == s else 0 for s in solutions]\n",
    "            y_pred = [1 if predicted == s else 0 for s in solutions]\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_true, y_pred),\n",
    "                'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "                'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "                'f1': f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            }\n",
    "        return predicted, top_5_ids, metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error predicting outcome for query '{query[:50]}...': {e}\")\n",
    "        return 'Tidak ditemukan', [], {}\n",
    "\n",
    "# Part 12: Main Function\n",
    "def main():\n",
    "    \"\"\"Main function to run prediction pipeline.\"\"\"\n",
    "    try:\n",
    "        setup_environment()\n",
    "        df, texts, case_ids, queries, case_solutions = load_data()\n",
    "        vectorizer, tfidf_matrix = setup_tfidf(texts)\n",
    "        X_train, y_train = prepare_training_data(queries, case_ids, texts, vectorizer, tfidf_matrix)\n",
    "        logreg, svm = train_models(X_train, y_train)\n",
    "        bi_encoder, cross_encoder, doc_embeddings, bm25 = setup_indobert_and_bm25(texts)\n",
    "        results_logreg = []\n",
    "        results_svm = []\n",
    "        results_indobert = []\n",
    "        for i in tqdm(range(len(df)), desc='Predicting'):\n",
    "            query_text = df.loc[i, 'ringkasan_fakta']\n",
    "            case_id = df.loc[i, 'case_id']\n",
    "            true_solution = case_solutions.get(case_id, '')\n",
    "            pred_logreg, top_ids_logreg, metrics_logreg = predict_outcome(\n",
    "                query_text,\n",
    "                lambda q: logreg_retrieve(q, vectorizer, tfidf_matrix, case_ids, texts, logreg),\n",
    "                case_solutions,\n",
    "                true_solution\n",
    "            )\n",
    "            results_logreg.append({\n",
    "                'query_id': case_id,\n",
    "                'predicted_solution': pred_logreg,\n",
    "                'top_5_case_ids': ', '.join(top_ids_logreg),\n",
    "                'metrics': metrics_logreg\n",
    "            })\n",
    "            pred_svm, top_ids_svm, metrics_svm = predict_outcome(\n",
    "                query_text,\n",
    "                lambda q: svm_retrieve(q, vectorizer, tfidf_matrix, case_ids, texts, svm),\n",
    "                case_solutions,\n",
    "                true_solution\n",
    "            )\n",
    "            results_svm.append({\n",
    "                'query_id': case_id,\n",
    "                'predicted_solution': pred_svm,\n",
    "                'top_5_case_ids': ', '.join(top_ids_svm),\n",
    "                'metrics': metrics_svm\n",
    "            })\n",
    "            pred_indobert, top_ids_indobert, metrics_indobert = predict_outcome(\n",
    "                query_text,\n",
    "                lambda q: indobert_retrieve(q, bi_encoder, cross_encoder, doc_embeddings, bm25, case_ids, texts),\n",
    "                case_solutions,\n",
    "                true_solution\n",
    "            )\n",
    "            results_indobert.append({\n",
    "                'query_id': case_id,\n",
    "                'predicted_solution': pred_indobert,\n",
    "                'top_5_case_ids': ', '.join(top_ids_indobert),\n",
    "                'metrics': metrics_indobert\n",
    "            })\n",
    "        pd.DataFrame(results_logreg).to_csv(\n",
    "            os.path.join(PATH_RESULTS, 'logreg_predictions.csv'), index=False, encoding='utf-8'\n",
    "        )\n",
    "        pd.DataFrame(results_svm).to_csv(\n",
    "            os.path.join(PATH_RESULTS, 'svm_predictions.csv'), index=False, encoding='utf-8'\n",
    "        )\n",
    "        pd.DataFrame(results_indobert).to_csv(\n",
    "            os.path.join(PATH_RESULTS, 'indobert_predictions.csv'), index=False, encoding='utf-8'\n",
    "        )\n",
    "        logger.info(f\"Saved results to {PATH_RESULTS}/logreg_predictions.csv\")\n",
    "        logger.info(f\"Saved results to {PATH_RESULTS}/svm_predictions.csv\")\n",
    "        logger.info(f\"Saved results to {PATH_RESULTS}/indobert_predictions.csv\")\n",
    "        print(f\"✅ Saved to {PATH_RESULTS}/logreg_predictions.csv\")\n",
    "        print(f\"✅ Saved to {PATH_RESULTS}/svm_predictions.csv\")\n",
    "        print(f\"✅ Saved to {PATH_RESULTS}/indobert_predictions.csv\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in main: {e}\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
