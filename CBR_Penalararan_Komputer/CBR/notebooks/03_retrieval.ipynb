{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33038ae",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c1b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bcff522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\notebooks\\venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "987c8c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/processed/cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ab297d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>nomor_perkara</th>\n",
       "      <th>tahun_putusan</th>\n",
       "      <th>bulan_putusan</th>\n",
       "      <th>tanggal_putusan</th>\n",
       "      <th>jenis_perkara</th>\n",
       "      <th>tingkat_pemeriksaan</th>\n",
       "      <th>lembaga_peradilan</th>\n",
       "      <th>pasal</th>\n",
       "      <th>hakim_ketua</th>\n",
       "      <th>ringkasan_fakta</th>\n",
       "      <th>jumlah_kata_putusan</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case_001</td>\n",
       "      <td>68 pid sus 2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>April</td>\n",
       "      <td>3</td>\n",
       "      <td>Pidana Khusus Kejahatan Siber</td>\n",
       "      <td>Pertama</td>\n",
       "      <td>Pengadilan Negeri</td>\n",
       "      <td>pasal 29, Undang-Undang Nomor 44 Tahun 2008</td>\n",
       "      <td>dengan didampingi hakim anggota abdul gafur bu...</td>\n",
       "      <td>menimbang bahwa terdakwa diajukan ke persidang...</td>\n",
       "      <td>8889</td>\n",
       "      <td>p u t u s a nnomor 68 pid sus 2014 pn bikdemi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case_002</td>\n",
       "      <td>103 pid 2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>September</td>\n",
       "      <td>5</td>\n",
       "      <td>Pidana Khusus Kejahatan Siber</td>\n",
       "      <td>Banding</td>\n",
       "      <td>Pengadilan Tinggi</td>\n",
       "      <td>pasal 233 ayat 2, Undang-Undang Nomor 8 Tahun ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bahwa terdakwa 1 oleksandr sulima alias oleks ...</td>\n",
       "      <td>6507</td>\n",
       "      <td>p u t u s a nnomor 103 pid 2016 pt dkidemi kea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case_003</td>\n",
       "      <td>1072 pid sus 2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>June</td>\n",
       "      <td>26</td>\n",
       "      <td>Pidana Khusus Kejahatan Siber</td>\n",
       "      <td>Banding</td>\n",
       "      <td>Mahkamah Agung</td>\n",
       "      <td>pasal 85, Undang-Undang Nomor 8 Tahun 1981</td>\n",
       "      <td>dengan didampingi para hakim anggotatersebut d...</td>\n",
       "      <td>menimbang bahwa terdakwa diajukan ke persidang...</td>\n",
       "      <td>22518</td>\n",
       "      <td>p u t u s a nnomor 1072 pid sus 2020 pn jkt ut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case_004</td>\n",
       "      <td>1074 pid sus 2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>June</td>\n",
       "      <td>26</td>\n",
       "      <td>Pidana Khusus Kejahatan Siber</td>\n",
       "      <td>Pertama</td>\n",
       "      <td>Pengadilan Negeri</td>\n",
       "      <td>pasal 85, Undang-Undang Nomor 8 Tahun 1981</td>\n",
       "      <td>dengan didampingi para hakim anggotatersebut d...</td>\n",
       "      <td>menimbang bahwa terdakwa diajukan ke persidang...</td>\n",
       "      <td>28927</td>\n",
       "      <td>p u t u s a nnomor 1074 pid sus 2020 pn jkt ut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case_005</td>\n",
       "      <td>10 pid sus 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>October</td>\n",
       "      <td>1</td>\n",
       "      <td>Pidana Khusus Kejahatan Siber</td>\n",
       "      <td>Pertama</td>\n",
       "      <td>Mahkamah Agung</td>\n",
       "      <td>pasal 30 ayat 1, Undang-Undang Nomor 11 Tahun ...</td>\n",
       "      <td>dengan didampingi para hakim anggota tersebut ...</td>\n",
       "      <td>menimbang bahwa terdakwa diajukan ke persidang...</td>\n",
       "      <td>15726</td>\n",
       "      <td>dari 48 putusan nomor 10 pid sus 2021 pn pli p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    case_id      nomor_perkara  tahun_putusan bulan_putusan  tanggal_putusan  \\\n",
       "0  case_001    68 pid sus 2014           2014         April                3   \n",
       "1  case_002       103 pid 2016           2016     September                5   \n",
       "2  case_003  1072 pid sus 2020           2020          June               26   \n",
       "3  case_004  1074 pid sus 2020           2020          June               26   \n",
       "4  case_005    10 pid sus 2021           2021       October                1   \n",
       "\n",
       "                   jenis_perkara tingkat_pemeriksaan  lembaga_peradilan  \\\n",
       "0  Pidana Khusus Kejahatan Siber             Pertama  Pengadilan Negeri   \n",
       "1  Pidana Khusus Kejahatan Siber             Banding  Pengadilan Tinggi   \n",
       "2  Pidana Khusus Kejahatan Siber             Banding     Mahkamah Agung   \n",
       "3  Pidana Khusus Kejahatan Siber             Pertama  Pengadilan Negeri   \n",
       "4  Pidana Khusus Kejahatan Siber             Pertama     Mahkamah Agung   \n",
       "\n",
       "                                               pasal  \\\n",
       "0        pasal 29, Undang-Undang Nomor 44 Tahun 2008   \n",
       "1  pasal 233 ayat 2, Undang-Undang Nomor 8 Tahun ...   \n",
       "2         pasal 85, Undang-Undang Nomor 8 Tahun 1981   \n",
       "3         pasal 85, Undang-Undang Nomor 8 Tahun 1981   \n",
       "4  pasal 30 ayat 1, Undang-Undang Nomor 11 Tahun ...   \n",
       "\n",
       "                                         hakim_ketua  \\\n",
       "0  dengan didampingi hakim anggota abdul gafur bu...   \n",
       "1                                                NaN   \n",
       "2  dengan didampingi para hakim anggotatersebut d...   \n",
       "3  dengan didampingi para hakim anggotatersebut d...   \n",
       "4  dengan didampingi para hakim anggota tersebut ...   \n",
       "\n",
       "                                     ringkasan_fakta  jumlah_kata_putusan  \\\n",
       "0  menimbang bahwa terdakwa diajukan ke persidang...                 8889   \n",
       "1  bahwa terdakwa 1 oleksandr sulima alias oleks ...                 6507   \n",
       "2  menimbang bahwa terdakwa diajukan ke persidang...                22518   \n",
       "3  menimbang bahwa terdakwa diajukan ke persidang...                28927   \n",
       "4  menimbang bahwa terdakwa diajukan ke persidang...                15726   \n",
       "\n",
       "                                           full_text  \n",
       "0  p u t u s a nnomor 68 pid sus 2014 pn bikdemi ...  \n",
       "1  p u t u s a nnomor 103 pid 2016 pt dkidemi kea...  \n",
       "2  p u t u s a nnomor 1072 pid sus 2020 pn jkt ut...  \n",
       "3  p u t u s a nnomor 1074 pid sus 2020 pn jkt ut...  \n",
       "4  dari 48 putusan nomor 10 pid sus 2021 pn pli p...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3509fd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_id                 0\n",
       "nomor_perkara          17\n",
       "tahun_putusan           0\n",
       "bulan_putusan           0\n",
       "tanggal_putusan         0\n",
       "jenis_perkara           0\n",
       "tingkat_pemeriksaan     0\n",
       "lembaga_peradilan       0\n",
       "pasal                   2\n",
       "hakim_ketua            11\n",
       "ringkasan_fakta        24\n",
       "jumlah_kata_putusan     0\n",
       "full_text               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfa1078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_27732\\87438782.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['hakim_ketua'].fillna(df['hakim_ketua'].mode()[0], inplace=True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_27732\\87438782.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['nomor_perkara'].fillna(df['nomor_perkara'].mode()[0], inplace=True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_27732\\87438782.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['pasal'].fillna(df['pasal'].mode()[0], inplace=True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_27732\\87438782.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['ringkasan_fakta'].fillna(df['ringkasan_fakta'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df['hakim_ketua'].fillna(df['hakim_ketua'].mode()[0], inplace=True)\n",
    "df['nomor_perkara'].fillna(df['nomor_perkara'].mode()[0], inplace=True)\n",
    "df['pasal'].fillna(df['pasal'].mode()[0], inplace=True)\n",
    "df['ringkasan_fakta'].fillna(df['ringkasan_fakta'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a08ad84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_id                0\n",
       "nomor_perkara          0\n",
       "tahun_putusan          0\n",
       "bulan_putusan          0\n",
       "tanggal_putusan        0\n",
       "jenis_perkara          0\n",
       "tingkat_pemeriksaan    0\n",
       "lembaga_peradilan      0\n",
       "pasal                  0\n",
       "hakim_ketua            0\n",
       "ringkasan_fakta        0\n",
       "jumlah_kata_putusan    0\n",
       "full_text              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde04c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['nomor_perkara', 'tahun_putusan', 'bulan_putusan', 'tanggal_putusan', 'jenis_perkara', 'tingkat_pemeriksaan', 'lembaga_peradilan', 'hakim_ketua', 'pasal', 'jumlah_kata_putusan', 'full_text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "578d6a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67 entries, 0 to 66\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   case_id          67 non-null     object\n",
      " 1   ringkasan_fakta  67 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6db5a1",
   "metadata": {},
   "source": [
    "# Text Representation & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dd309",
   "metadata": {},
   "source": [
    "## Kolom ringkasan_fakta & case_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac9ed2",
   "metadata": {},
   "source": [
    "### TF-IDF (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "174f14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:01:17,504 - INFO - Starting retrieval process at 2025-06-26 22:01:17\n",
      "2025-06-26 22:01:17,506 - WARNING - d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\processed\\cases.csv not found. Creating sample cases.csv.\n",
      "2025-06-26 22:01:17,513 - INFO - Created sample d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\processed\\cases.csv with 50 cases.\n",
      "2025-06-26 22:01:17,515 - WARNING - d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\eval\\queries.json not found. Creating sample queries.json.\n",
      "2025-06-26 22:01:17,516 - INFO - Created sample d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\eval\\queries.json with 5 queries.\n",
      "2025-06-26 22:01:17,532 - INFO - Loaded 50 cases from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\processed\\cases.csv\n",
      "2025-06-26 22:01:17,540 - INFO - TF-IDF matrix shape: (50, 267)\n",
      "2025-06-26 22:01:17,553 - INFO - Loaded 5 evaluation queries from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\eval\\queries.json\n",
      "2025-06-26 22:01:17,615 - INFO - Training data shape: (100, 537)\n",
      "2025-06-26 22:01:17,617 - INFO - Class distribution: [50 50]\n",
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\notebooks\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\notebooks\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\CBR\\notebooks\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "2025-06-26 22:01:17,744 - INFO - Best SVM Parameters: {'C': 0.01}\n",
      "2025-06-26 22:01:17,745 - INFO - Best CV Accuracy: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (100, 537)\n",
      "Class distribution: [50 50]\n",
      "Best SVM Parameters: {'C': 0.01}\n",
      "Best CV Accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:01:17,836 - INFO - Query: Legal dispute case 1...\n",
      "2025-06-26 22:01:17,838 - INFO - Top 5 Results (ID, Score): [('CASE_1', np.float64(0.8911614142838018)), ('CASE_2', np.float64(0.7756505588129035)), ('CASE_3', np.float64(0.7756505588129035)), ('CASE_4', np.float64(0.7756505588129035)), ('CASE_5', np.float64(0.7756505588129035))]\n",
      "2025-06-26 22:01:17,839 - INFO - FOUND in Top-5: CASE_1\n",
      "2025-06-26 22:01:17,896 - INFO - Query: Legal dispute case 2...\n",
      "2025-06-26 22:01:17,897 - INFO - Top 5 Results (ID, Score): [('CASE_2', np.float64(0.8911614142838018)), ('CASE_1', np.float64(0.7756505588129035)), ('CASE_3', np.float64(0.7756505588129035)), ('CASE_4', np.float64(0.7756505588129035)), ('CASE_5', np.float64(0.7756505588129035))]\n",
      "2025-06-26 22:01:17,898 - INFO - FOUND in Top-5: CASE_2\n",
      "2025-06-26 22:01:17,953 - INFO - Query: Legal dispute case 3...\n",
      "2025-06-26 22:01:17,954 - INFO - Top 5 Results (ID, Score): [('CASE_3', np.float64(0.8911614142838018)), ('CASE_1', np.float64(0.7756505588129035)), ('CASE_2', np.float64(0.7756505588129035)), ('CASE_4', np.float64(0.7756505588129035)), ('CASE_5', np.float64(0.7756505588129035))]\n",
      "2025-06-26 22:01:17,955 - INFO - FOUND in Top-5: CASE_3\n",
      "2025-06-26 22:01:18,017 - INFO - Query: Legal dispute case 4...\n",
      "2025-06-26 22:01:18,018 - INFO - Top 5 Results (ID, Score): [('CASE_4', np.float64(0.8911614142838018)), ('CASE_1', np.float64(0.7756505588129035)), ('CASE_2', np.float64(0.7756505588129035)), ('CASE_3', np.float64(0.7756505588129035)), ('CASE_5', np.float64(0.7756505588129035))]\n",
      "2025-06-26 22:01:18,018 - INFO - FOUND in Top-5: CASE_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Legal dispute case 1\n",
      "Top 5 Results (ID, Score): [('CASE_1', np.float64(0.8911614142838018)), ('CASE_2', np.float64(0.7756505588129035)), ('CASE_3', np.float64(0.7756505588129035)), ('CASE_4', np.float64(0.7756505588129035)), ('CASE_5', np.float64(0.7756505588129035))]\n",
      "✅ FOUND in Top-5: CASE_1\n",
      "\n",
      "Query: Legal dispute case 2\n",
      "Top 5 Results (ID, Score): [('CASE_2', np.float64(0.8911614142838018)), ('CASE_1', np.float64(0.7756505588129035)), ('CASE_3', np.float64(0.7756505588129035)), ('CASE_4', np.float64(0.7756505588129035)), ('CASE_5', np.float64(0.7756505588129035))]\n",
      "✅ FOUND in Top-5: CASE_2\n",
      "\n",
      "Query: Legal dispute case 3\n",
      "Top 5 Results (ID, Score): [('CASE_3', np.float64(0.8911614142838018)), ('CASE_1', np.float64(0.7756505588129035)), ('CASE_2', np.float64(0.7756505588129035)), ('CASE_4', np.float64(0.7756505588129035)), ('CASE_5', np.float64(0.7756505588129035))]\n",
      "✅ FOUND in Top-5: CASE_3\n",
      "\n",
      "Query: Legal dispute case 4\n",
      "Top 5 Results (ID, Score): [('CASE_4', np.float64(0.8911614142838018)), ('CASE_1', np.float64(0.7756505588129035)), ('CASE_2', np.float64(0.7756505588129035)), ('CASE_3', np.float64(0.7756505588129035)), ('CASE_5', np.float64(0.7756505588129035))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:01:18,080 - INFO - Query: Legal dispute case 5...\n",
      "2025-06-26 22:01:18,081 - INFO - Top 5 Results (ID, Score): [('CASE_5', np.float64(0.8911614142838018)), ('CASE_1', np.float64(0.7756505588129035)), ('CASE_2', np.float64(0.7756505588129035)), ('CASE_3', np.float64(0.7756505588129035)), ('CASE_4', np.float64(0.7756505588129035))]\n",
      "2025-06-26 22:01:18,082 - INFO - FOUND in Top-5: CASE_5\n",
      "2025-06-26 22:01:18,083 - INFO - SVM Pairwise Accuracy@5: 5/5 = 1.00\n",
      "2025-06-26 22:01:18,084 - INFO - SVM Pairwise Recall@10: 5/5 = 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FOUND in Top-5: CASE_4\n",
      "\n",
      "Query: Legal dispute case 5\n",
      "Top 5 Results (ID, Score): [('CASE_5', np.float64(0.8911614142838018)), ('CASE_1', np.float64(0.7756505588129035)), ('CASE_2', np.float64(0.7756505588129035)), ('CASE_3', np.float64(0.7756505588129035)), ('CASE_4', np.float64(0.7756505588129035))]\n",
      "✅ FOUND in Top-5: CASE_5\n",
      "\n",
      "SVM Pairwise Accuracy@5: 5/5 = 1.00\n",
      "SVM Pairwise Recall@10: 5/5 = 1.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define base directory\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.getcwd())  # Parent of 'notebooks'\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # Fallback for interactive environments\n",
    "\n",
    "# Define paths\n",
    "PATH_CSV = os.path.join(BASE_DIR, 'data', 'processed', 'cases.csv')\n",
    "PATH_QUERIES = os.path.join(BASE_DIR, 'data', 'eval', 'queries.json')\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "LOG_PATH = os.path.join(LOG_DIR, 'retrieval.log')\n",
    "\n",
    "# Validate path length for Windows\n",
    "MAX_PATH_LENGTH = 260\n",
    "\n",
    "def validate_path(path):\n",
    "    if len(path) > MAX_PATH_LENGTH:\n",
    "        raise ValueError(f\"Path {path} exceeds Windows maximum length of {MAX_PATH_LENGTH} characters\")\n",
    "    return path\n",
    "\n",
    "# Ensure directories exist\n",
    "for path in [LOG_DIR, os.path.dirname(PATH_CSV), os.path.dirname(PATH_QUERIES)]:\n",
    "    try:\n",
    "        validate_path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logging.info(f\"Directory ensured: {path}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Path validation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH, mode='w', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logging.info(\"Starting retrieval process at %s\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Create sample data if files are missing\n",
    "if not os.path.exists(PATH_CSV):\n",
    "    logging.warning(f\"{PATH_CSV} not found. Creating sample cases.csv.\")\n",
    "    sample_cases = pd.DataFrame({\n",
    "        \"case_id\": [f\"CASE_{i}\" for i in range(1, 51)],\n",
    "        \"ringkasan_fakta\": [f\"Sample case description {i} about legal proceedings and disputes.\" for i in range(1, 51)]\n",
    "    })\n",
    "    sample_cases.to_csv(PATH_CSV, index=False)\n",
    "    logging.info(f\"Created sample {PATH_CSV} with {len(sample_cases)} cases.\")\n",
    "\n",
    "if not os.path.exists(PATH_QUERIES):\n",
    "    logging.warning(f\"{PATH_QUERIES} not found. Creating sample queries.json.\")\n",
    "    sample_queries = [\n",
    "        {\"query\": f\"Legal dispute case {i}\", \"case_id\": f\"CASE_{i}\"}\n",
    "        for i in range(1, 6)\n",
    "    ]\n",
    "    with open(PATH_QUERIES, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_queries, f, indent=2)\n",
    "    logging.info(f\"Created sample {PATH_QUERIES} with {len(sample_queries)} queries.\")\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocess text by lowercasing, removing punctuation, and normalizing spaces.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Feature Extraction\n",
    "def extract_features(query_vec, doc_vec, query_text: str, doc_text: str) -> np.ndarray:\n",
    "    \"\"\"Extract features including TF-IDF vectors, cosine similarity, word overlap, and coverage.\"\"\"\n",
    "    query_vec = query_vec.toarray()[0]\n",
    "    doc_vec = doc_vec.toarray()[0]\n",
    "    cos_sim = cosine_similarity([query_vec], [doc_vec])[0][0]\n",
    "    query_words = set(query_text.split())\n",
    "    doc_words = set(doc_text.split())\n",
    "    overlap = len(query_words.intersection(doc_words)) / max(len(query_words), 1)\n",
    "    coverage = len(query_words.intersection(doc_words)) / max(len(query_words), 1)\n",
    "    combined_vec = np.concatenate([query_vec, doc_vec, [cos_sim, overlap, coverage]])\n",
    "    return combined_vec\n",
    "\n",
    "# Load and Prepare Data\n",
    "try:\n",
    "    df = pd.read_csv(PATH_CSV)\n",
    "    logging.info(f\"Loaded {len(df)} cases from {PATH_CSV}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"File {PATH_CSV} not found.\")\n",
    "    print(f\"Error: File {PATH_CSV} not found.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading {PATH_CSV}: {e}\")\n",
    "    print(f\"Error loading {PATH_CSV}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "texts = df[\"ringkasan_fakta\"].fillna(\"\").apply(preprocess_text)\n",
    "case_ids = df[\"case_id\"].tolist()\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "id_stop_words = [\n",
    "    \"dan\", \"di\", \"dari\", \"ke\", \"pada\", \"dengan\", \"untuk\", \"yang\", \"ini\", \"itu\",\n",
    "    \"adalah\", \"tersebut\", \"sebagai\", \"oleh\", \"atau\", \"tetapi\", \"karena\", \"jika\",\n",
    "    \"dalam\", \"bagi\", \"tentang\", \"melalui\", \"serta\", \"maka\", \"lagi\", \"sudah\",\n",
    "    \"belum\", \"hanya\", \"saja\", \"bahwa\", \"apa\", \"siapa\", \"bagaimana\", \"kapan\",\n",
    "    \"dimana\", \"kenapa\", \"sejak\", \"hingga\", \"agar\", \"supaya\", \"meskipun\", \"walau\",\n",
    "    \"kecuali\", \"terhadap\", \"antara\", \"selain\", \"setiap\", \"sebelum\", \"sesudah\"\n",
    "]\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=4000,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=id_stop_words\n",
    ")\n",
    "try:\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    logging.info(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error in TF-IDF vectorization: {e}\")\n",
    "    print(f\"Error in TF-IDF vectorization: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load evaluation queries\n",
    "try:\n",
    "    with open(PATH_QUERIES, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_queries = json.load(f)\n",
    "    logging.info(f\"Loaded {len(eval_queries)} evaluation queries from {PATH_QUERIES}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"File {PATH_QUERIES} not found.\")\n",
    "    print(f\"Error: File {PATH_QUERIES} not found.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading {PATH_QUERIES}: {e}\")\n",
    "    print(f\"Error loading {PATH_QUERIES}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Train Pairwise SVM\n",
    "if not eval_queries:\n",
    "    logging.error(\"No evaluation queries loaded. Cannot proceed with training.\")\n",
    "    print(\"Error: No evaluation queries loaded.\")\n",
    "    exit(1)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for item in eval_queries:\n",
    "    query = preprocess_text(item[\"query\"])\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    try:\n",
    "        true_id = item[\"case_id\"]\n",
    "        true_idx = case_ids.index(true_id)\n",
    "    except ValueError:\n",
    "        logging.warning(f\"Case ID {true_id} not found in dataset. Skipping query: {item['query'][:50]}...\")\n",
    "        continue\n",
    "\n",
    "    true_vec = tfidf_matrix[true_idx]\n",
    "    pos_features = extract_features(query_vec, true_vec, query, texts[true_idx])\n",
    "\n",
    "    neg_indices = [i for i in range(len(case_ids)) if i != true_idx]\n",
    "    neg_samples = np.random.choice(neg_indices, size=min(10, len(neg_indices)), replace=False)\n",
    "\n",
    "    for neg_idx in neg_samples:\n",
    "        neg_vec = tfidf_matrix[neg_idx]\n",
    "        neg_features = extract_features(query_vec, neg_vec, query, texts[neg_idx])\n",
    "        X_train.append(pos_features - neg_features)\n",
    "        y_train.append(1)\n",
    "        X_train.append(neg_features - pos_features)\n",
    "        y_train.append(0)\n",
    "\n",
    "if not X_train:\n",
    "    logging.error(\"No training data generated. Cannot proceed with SVM training.\")\n",
    "    print(\"Error: No training data generated.\")\n",
    "    exit(1)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "logging.info(f\"Training data shape: {X_train.shape}\")\n",
    "logging.info(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "\n",
    "# Train SVM\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "svm = GridSearchCV(\n",
    "    LinearSVC(max_iter=1000, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "try:\n",
    "    svm.fit(X_train, y_train)\n",
    "    logging.info(f\"Best SVM Parameters: {svm.best_params_}\")\n",
    "    logging.info(f\"Best CV Accuracy: {svm.best_score_:.2f}\")\n",
    "    print(f\"Best SVM Parameters: {svm.best_params_}\")\n",
    "    print(f\"Best CV Accuracy: {svm.best_score_:.2f}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error training SVM: {e}\")\n",
    "    print(f\"Error training SVM: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Retrieval Function\n",
    "def retrieve(query: str, k: int = 5) -> List[tuple]:\n",
    "    \"\"\"Retrieve top-k cases for a given query using SVM scoring.\"\"\"\n",
    "    query = preprocess_text(query)\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = []\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        doc_vec = tfidf_matrix[i]\n",
    "        features = extract_features(query_vec, doc_vec, query, texts[i])\n",
    "        score = svm.decision_function([features])[0]\n",
    "        scores.append((case_ids[i], score))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:k]\n",
    "\n",
    "# Evaluation\n",
    "correct_5 = 0\n",
    "correct_10 = 0\n",
    "for item in eval_queries:\n",
    "    query = item[\"query\"]\n",
    "    true_id = item[\"case_id\"]\n",
    "    try:\n",
    "        results = retrieve(query, k=10)\n",
    "        top_5 = results[:5]\n",
    "        logging.info(f\"Query: {query[:100]}...\")\n",
    "        logging.info(f\"Top 5 Results (ID, Score): {top_5}\")\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"Top 5 Results (ID, Score): {top_5}\")\n",
    "\n",
    "        if any(true_id == result[0] for result in top_5):\n",
    "            logging.info(f\"FOUND in Top-5: {true_id}\")\n",
    "            print(f\"✅ FOUND in Top-5: {true_id}\")\n",
    "            correct_5 += 1\n",
    "        else:\n",
    "            logging.warning(f\"NOT FOUND in Top-5: {true_id}\")\n",
    "            print(f\"❌ NOT FOUND in Top-5: {true_id}\")\n",
    "            try:\n",
    "                true_idx = case_ids.index(true_id)\n",
    "                true_text = texts[true_idx][:300] + \"...\" if len(texts[true_idx]) > 300 else texts[true_idx]\n",
    "                logging.info(f\"True Case Text: {true_text}\")\n",
    "                print(f\"True Case Text: {true_text}\")\n",
    "                query_words = set(preprocess_text(query).split())\n",
    "                true_words = set(true_text.split())\n",
    "                common_words = query_words.intersection(true_words)\n",
    "                logging.info(f\"Common Words: {common_words}\")\n",
    "                print(f\"Common Words: {common_words}\")\n",
    "                query_vec = vectorizer.transform([query])\n",
    "                true_vec = tfidf_matrix[true_idx]\n",
    "                cos_sim = cosine_similarity(query_vec, true_vec)[0][0]\n",
    "                logging.info(f\"Cosine Similarity with True Doc: {cos_sim:.4f}\")\n",
    "                print(f\"Cosine Similarity with True Doc: {cos_sim:.4f}\")\n",
    "            except ValueError:\n",
    "                logging.warning(f\"True case ID {true_id} not found in dataset.\")\n",
    "\n",
    "        if any(true_id == result[0] for result in results):\n",
    "            correct_10 += 1\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error evaluating query '{query[:50]}...': {e}\")\n",
    "        print(f\"Error evaluating query: {e}\")\n",
    "        continue\n",
    "\n",
    "logging.info(f\"SVM Pairwise Accuracy@5: {correct_5}/{len(eval_queries)} = {correct_5 / len(eval_queries):.2f}\")\n",
    "logging.info(f\"SVM Pairwise Recall@10: {correct_10}/{len(eval_queries)} = {correct_10 / len(eval_queries):.2f}\")\n",
    "print(f\"\\nSVM Pairwise Accuracy@5: {correct_5}/{len(eval_queries)} = {correct_5 / len(eval_queries):.2f}\")\n",
    "print(f\"SVM Pairwise Recall@10: {correct_10}/{len(eval_queries)} = {correct_10 / len(eval_queries):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f205591",
   "metadata": {},
   "source": [
    "### Indo-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8b6bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (0.33.1)\n",
      "Requirement already satisfied: Pillow in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: filelock in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: sympy in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.6.15)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (2.1.2)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: torch in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.14.0)\n",
      "Requirement already satisfied: sympy in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: colorama in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras)\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.14.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached grpcio-1.73.1-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.2)\n",
      "Collecting h5py>=3.11.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached h5py-3.14.0-cp310-cp310-win_amd64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl.metadata (22 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.6.15)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached optree-0.16.0-cp310-cp310-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.19.0-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "Using cached grpcio-1.73.1-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl (209 kB)\n",
      "Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.14.0-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.16.0-cp310-cp310-win_amd64.whl (304 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow, tf-keras\n",
      "\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   -- -------------------------------------  2/27 [flatbuffers]\n",
      "   ---- -----------------------------------  3/27 [wrapt]\n",
      "   ---- -----------------------------------  3/27 [wrapt]\n",
      "   ----- ----------------------------------  4/27 [wheel]\n",
      "   ----- ----------------------------------  4/27 [wheel]\n",
      "   ----- ----------------------------------  4/27 [wheel]\n",
      "   ----- ----------------------------------  4/27 [wheel]\n",
      "   ----- ----------------------------------  4/27 [wheel]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ---------- ----------------------------  7/27 [tensorflow-io-gcs-filesystem]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   -------------- ------------------------- 10/27 [optree]\n",
      "   -------------- ------------------------- 10/27 [optree]\n",
      "   -------------- ------------------------- 10/27 [optree]\n",
      "   -------------- ------------------------- 10/27 [optree]\n",
      "   ---------------- ----------------------- 11/27 [opt-einsum]\n",
      "   ---------------- ----------------------- 11/27 [opt-einsum]\n",
      "   ---------------- ----------------------- 11/27 [opt-einsum]\n",
      "   ---------------- ----------------------- 11/27 [opt-einsum]\n",
      "   ----------------- ---------------------- 12/27 [ml-dtypes]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ---------------------- ----------------- 15/27 [h5py]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [grpcio]\n",
      "   ------------------------- -------------- 17/27 [google-pasta]\n",
      "   ------------------------- -------------- 17/27 [google-pasta]\n",
      "   ------------------------- -------------- 17/27 [google-pasta]\n",
      "   -------------------------- ------------- 18/27 [gast]\n",
      "   ---------------------------- ----------- 19/27 [absl-py]\n",
      "   ---------------------------- ----------- 19/27 [absl-py]\n",
      "   ---------------------------- ----------- 19/27 [absl-py]\n",
      "   ---------------------------- ----------- 19/27 [absl-py]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ----------------------------- ---------- 20/27 [tensorboard]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   -------------------------------- ------- 22/27 [astunparse]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ---------------------------------- ----- 23/27 [rich]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ----------------------------------- ---- 24/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   ------------------------------------- -- 25/27 [tensorflow]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   -------------------------------------- - 26/27 [tf-keras]\n",
      "   ---------------------------------------- 27/27 [tf-keras]\n",
      "\n",
      "Successfully installed absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 tf-keras-2.19.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: rank-bm25 in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in d:\\tugas smt 6\\penalaran komputer\\tugas akhir uas\\code\\penalaran_komputer\\cbr_penalararan_komputer(8)\\cbr\\notebooks\\venv\\lib\\site-packages (from rank-bm25) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers\n",
    "%pip install pandas numpy sklearn sentence-transformers torch tqdm\n",
    "%pip install transformers torch\n",
    "%pip install tf-keras\n",
    "%pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b8bbb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "2025-06-27 00:21:57,002 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\logs\n",
      "2025-06-27 00:21:57,004 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\processed\n",
      "2025-06-27 00:21:57,005 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\eval\n",
      "2025-06-27 00:21:57,009 - INFO - Starting retrieval process at 2025-06-27 00:21:57\n",
      "2025-06-27 00:21:57,014 - INFO - Loading SentenceTransformer model: indobenchmark/indobert-base-p1 on cpu\n",
      "2025-06-27 00:21:57,019 - INFO - Load pretrained SentenceTransformer: indobenchmark/indobert-base-p1\n",
      "2025-06-27 00:21:57,530 - WARNING - No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n",
      "2025-06-27 00:21:59,877 - INFO - Loading CrossEncoder model: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\n",
      "2025-06-27 00:22:03,093 - INFO - Loading dataset from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\processed\\cases.csv\n",
      "2025-06-27 00:22:03,098 - INFO - Loaded 50 cases from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\processed\\cases.csv\n",
      "2025-06-27 00:22:03,099 - INFO - Preprocessing texts\n",
      "2025-06-27 00:22:03,108 - INFO - Initializing BM25Okapi with k1=1.5, b=0.75\n",
      "2025-06-27 00:22:03,109 - INFO - Encoding documents\n",
      "Encoding documents: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n",
      "2025-06-27 00:22:04,017 - INFO - Document embeddings shape: (50, 768)\n",
      "2025-06-27 00:22:04,020 - INFO - Loading evaluation queries from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\eval\\queries.json\n",
      "2025-06-27 00:22:04,022 - INFO - Loaded 5 evaluation queries\n",
      "2025-06-27 00:22:04,024 - INFO - Evaluating with alpha=0.4\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.33it/s]\n",
      "2025-06-27 00:22:04,958 - INFO - Alpha=0.4: Accuracy@5=1.00, Recall@10=1.00, MRR=1.00\n",
      "2025-06-27 00:22:04,958 - INFO - Evaluating with alpha=0.5\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.76it/s]\n",
      "2025-06-27 00:22:05,662 - INFO - Alpha=0.5: Accuracy@5=1.00, Recall@10=1.00, MRR=1.00\n",
      "2025-06-27 00:22:05,663 - INFO - Evaluating with alpha=0.6\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.91it/s]\n",
      "2025-06-27 00:22:06,383 - INFO - Alpha=0.6: Accuracy@5=1.00, Recall@10=1.00, MRR=1.00\n",
      "2025-06-27 00:22:06,384 - INFO - Evaluating with alpha=0.7\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n",
      "2025-06-27 00:22:07,103 - INFO - Alpha=0.7: Accuracy@5=1.00, Recall@10=1.00, MRR=1.00\n",
      "2025-06-27 00:22:07,104 - INFO - Evaluating with alpha=0.8\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.96it/s]\n",
      "2025-06-27 00:22:07,833 - INFO - Alpha=0.8: Accuracy@5=1.00, Recall@10=1.00, MRR=1.00\n",
      "2025-06-27 00:22:07,835 - INFO - Evaluating with alpha=0.9\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.53it/s]\n",
      "2025-06-27 00:22:08,650 - INFO - Alpha=0.9: Accuracy@5=1.00, Recall@10=1.00, MRR=1.00\n",
      "2025-06-27 00:22:08,651 - INFO - Best alpha: 0.4 with Accuracy@5: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.4 with Accuracy@5: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.51it/s]/s]\n",
      "2025-06-27 00:22:08,815 - INFO - Query: Legal dispute case 1...\n",
      "2025-06-27 00:22:08,816 - INFO - Top 5 Results (ID, Score): [('CASE_1', 2.608067512512207), ('CASE_36', -0.20904992520809174), ('CASE_18', -0.29714056849479675), ('CASE_33', -0.406864732503891), ('CASE_14', -0.4097851812839508)]\n",
      "2025-06-27 00:22:08,819 - INFO - ✅ FOUND in Top-5: CASE_1\n",
      "Evaluating queries:  20%|██        | 1/5 [00:00<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Legal dispute case 1\n",
      "Top 5 Results (ID, Score): [('CASE_1', 2.608067512512207), ('CASE_36', -0.20904992520809174), ('CASE_18', -0.29714056849479675), ('CASE_33', -0.406864732503891), ('CASE_14', -0.4097851812839508)]\n",
      "✅ FOUND in Top-5: CASE_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n",
      "2025-06-27 00:22:08,977 - INFO - Query: Legal dispute case 2...\n",
      "2025-06-27 00:22:08,978 - INFO - Top 5 Results (ID, Score): [('CASE_2', 3.8508942127227783), ('CASE_22', -0.8423956036567688), ('CASE_36', -0.8646412491798401), ('CASE_14', -0.9724110960960388), ('CASE_33', -1.0700106620788574)]\n",
      "2025-06-27 00:22:08,979 - INFO - ✅ FOUND in Top-5: CASE_2\n",
      "Evaluating queries:  40%|████      | 2/5 [00:00<00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Legal dispute case 2\n",
      "Top 5 Results (ID, Score): [('CASE_2', 3.8508942127227783), ('CASE_22', -0.8423956036567688), ('CASE_36', -0.8646412491798401), ('CASE_14', -0.9724110960960388), ('CASE_33', -1.0700106620788574)]\n",
      "✅ FOUND in Top-5: CASE_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "2025-06-27 00:22:09,276 - INFO - Query: Legal dispute case 3...\n",
      "2025-06-27 00:22:09,277 - INFO - Top 5 Results (ID, Score): [('CASE_3', 4.667585372924805), ('CASE_36', -1.0952690839767456), ('CASE_34', -1.1280572414398193), ('CASE_33', -1.183658480644226), ('CASE_35', -1.4974206686019897)]\n",
      "2025-06-27 00:22:09,278 - INFO - ✅ FOUND in Top-5: CASE_3\n",
      "Evaluating queries:  60%|██████    | 3/5 [00:00<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Legal dispute case 3\n",
      "Top 5 Results (ID, Score): [('CASE_3', 4.667585372924805), ('CASE_36', -1.0952690839767456), ('CASE_34', -1.1280572414398193), ('CASE_33', -1.183658480644226), ('CASE_35', -1.4974206686019897)]\n",
      "✅ FOUND in Top-5: CASE_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n",
      "2025-06-27 00:22:09,429 - INFO - Query: Legal dispute case 4...\n",
      "2025-06-27 00:22:09,430 - INFO - Top 5 Results (ID, Score): [('CASE_4', 5.204360008239746), ('CASE_17', -1.2443318367004395), ('CASE_6', -1.426748275756836), ('CASE_5', -1.6177444458007812), ('CASE_3', -1.6930291652679443)]\n",
      "2025-06-27 00:22:09,432 - INFO - ✅ FOUND in Top-5: CASE_4\n",
      "Evaluating queries:  80%|████████  | 4/5 [00:00<00:00,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Legal dispute case 4\n",
      "Top 5 Results (ID, Score): [('CASE_4', 5.204360008239746), ('CASE_17', -1.2443318367004395), ('CASE_6', -1.426748275756836), ('CASE_5', -1.6177444458007812), ('CASE_3', -1.6930291652679443)]\n",
      "✅ FOUND in Top-5: CASE_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "2025-06-27 00:22:09,592 - INFO - Query: Legal dispute case 5...\n",
      "2025-06-27 00:22:09,593 - INFO - Top 5 Results (ID, Score): [('CASE_5', 5.066492080688477), ('CASE_17', -1.1091742515563965), ('CASE_15', -1.138985514640808), ('CASE_6', -1.1803070306777954), ('CASE_18', -1.1884934902191162)]\n",
      "2025-06-27 00:22:09,595 - INFO - ✅ FOUND in Top-5: CASE_5\n",
      "Evaluating queries: 100%|██████████| 5/5 [00:00<00:00,  5.30it/s]\n",
      "2025-06-27 00:22:09,600 - INFO - IndoBERT-Base-P1 Accuracy@5: 5/5 = 1.00\n",
      "2025-06-27 00:22:09,601 - INFO - IndoBERT-Base-P1 Recall@10: 5/5 = 1.00\n",
      "2025-06-27 00:22:09,604 - INFO - IndoBERT-Base-P1 MRR: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Legal dispute case 5\n",
      "Top 5 Results (ID, Score): [('CASE_5', 5.066492080688477), ('CASE_17', -1.1091742515563965), ('CASE_15', -1.138985514640808), ('CASE_6', -1.1803070306777954), ('CASE_18', -1.1884934902191162)]\n",
      "✅ FOUND in Top-5: CASE_5\n",
      "\n",
      "IndoBERT-Base-P1 Accuracy@5: 5/5 = 1.00\n",
      "IndoBERT-Base-P1 Recall@10: 5/5 = 1.00\n",
      "IndoBERT-Base-P1 MRR: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Import Libraries and Initialize\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Define base directory (aligned with previous scripts)\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.getcwd())  # Parent of 'notebooks'\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # Fallback for interactive environments like Jupyter\n",
    "\n",
    "# Define paths\n",
    "PATH_CSV = os.path.join(BASE_DIR, 'data', 'processed', 'cases.csv')\n",
    "PATH_QUERIES = os.path.join(BASE_DIR, 'data', 'eval', 'queries.json')\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "LOG_PATH = os.path.join(LOG_DIR, 'retrieval_bert.log')\n",
    "\n",
    "# Validate path length for Windows\n",
    "MAX_PATH_LENGTH = 260\n",
    "\n",
    "def validate_path(path):\n",
    "    if len(path) > MAX_PATH_LENGTH:\n",
    "        raise ValueError(f\"Path {path} exceeds Windows maximum length of {MAX_PATH_LENGTH} characters\")\n",
    "    return path\n",
    "\n",
    "# Ensure directories exist\n",
    "for path in [LOG_DIR, os.path.dirname(PATH_CSV), os.path.dirname(PATH_QUERIES)]:\n",
    "    try:\n",
    "        validate_path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logging.info(f\"Directory ensured: {path}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Path validation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH, mode='w', encoding='utf-8'),  # Overwrite mode\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Starting retrieval process at %s\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    logger.info(\"Downloading NLTK punkt tokenizer\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    logger.info(\"Downloading NLTK stopwords\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Indonesian stopwords\n",
    "indo_stopwords = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Part 2: Load Models\n",
    "try:\n",
    "    model_name = \"indobenchmark/indobert-base-p1\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    logger.info(f\"Loading SentenceTransformer model: {model_name} on {device}\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    cross_encoder_model = \"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\"\n",
    "    logger.info(f\"Loading CrossEncoder model: {cross_encoder_model}\")\n",
    "    cross_encoder = CrossEncoder(cross_encoder_model, device=device)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load models: {e}\")\n",
    "    print(f\"Error: Failed to load models: {e}\")\n",
    "    raise\n",
    "\n",
    "# Part 3: Load Dataset\n",
    "try:\n",
    "    logger.info(f\"Loading dataset from {PATH_CSV}\")\n",
    "    df = pd.read_csv(PATH_CSV)\n",
    "    texts = df[\"ringkasan_fakta\"].fillna(\"\").tolist()\n",
    "    case_ids = df[\"case_id\"].tolist()\n",
    "    if not texts or not case_ids:\n",
    "        raise ValueError(\"Dataset is empty or missing required columns ('ringkasan_fakta', 'case_id').\")\n",
    "    logger.info(f\"Loaded {len(df)} cases from {PATH_CSV}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"File {PATH_CSV} not found. Please ensure 02_representation.py has run successfully.\")\n",
    "    print(f\"Error: File {PATH_CSV} not found.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load dataset: {e}\")\n",
    "    print(f\"Error loading {PATH_CSV}: {e}\")\n",
    "    raise\n",
    "\n",
    "# Part 4: Synonym Dictionary and Preprocessing\n",
    "synonym_dict = {\n",
    "    \"peretasan\": [\"retas\", \"akses ilegal\", \"akses tanpa izin\", \"penetrasi sistem\"],\n",
    "    \"malware\": [\"virus\", \"trojan\", \"ransomware\", \"keylogger\"],\n",
    "    \"hacking\": [\"pembobolan\", \"eksploitasi sistem\", \"crack\", \"bypass\"],\n",
    "    \"akun\": [\"rekening\", \"profil pengguna\", \"akses user\", \"login\"],\n",
    "    \"dokumen\": [\"arsip\", \"berkas\", \"file\", \"copy\"],\n",
    "    \"bank\": [\"bni\", \"mandiri\", \"bri\", \"rekening\"],\n",
    "    \"uang\": [\"rp\", \"rupiah\", \"transfer\", \"saldo\"],\n",
    "    \"penipuan\": [\"manipulasi\", \"pemalsuan\", \"modus\", \"rekayasa\"],\n",
    "    \"data\": [\"informasi\", \"rekaman\", \"file elektronik\", \"jejak digital\"],\n",
    "    \"sistem\": [\"aplikasi\", \"perangkat lunak\", \"server\", \"layanan\"]\n",
    "}\n",
    "\n",
    "def expand_query(query: str) -> str:\n",
    "    \"\"\"Expand query with synonyms for better recall.\"\"\"\n",
    "    words = query.split()\n",
    "    expanded_words = []\n",
    "    for word in words:\n",
    "        expanded_words.append(word)\n",
    "        if word in synonym_dict:\n",
    "            expanded_words.extend(synonym_dict[word])\n",
    "    return \" \".join(set(expanded_words))  # Remove duplicates\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocess text while preserving legal terms and removing stopwords.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    # Preserve legal phrases\n",
    "    text = re.sub(r'uu\\s+no', 'undang_undang_nomor', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'pasal\\s+\\d+', 'pasal', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t for t in tokens if t not in indo_stopwords or t in synonym_dict]\n",
    "        return ' '.join(tokens) if tokens else \"empty\"\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error tokenizing text: {e}. Returning empty string.\")\n",
    "        return \"empty\"\n",
    "\n",
    "logger.info(\"Preprocessing texts\")\n",
    "texts = [preprocess_text(text) for text in texts]\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "try:\n",
    "    logger.info(\"Initializing BM25Okapi with k1=1.5, b=0.75\")\n",
    "    bm25 = BM25Okapi(tokenized_texts, k1=1.5, b=0.75)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize BM25: {e}\")\n",
    "    print(f\"Error: Failed to initialize BM25: {e}\")\n",
    "    raise\n",
    "\n",
    "# Part 5: Encode Documents\n",
    "def encode_documents(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"Encode documents in batches.\"\"\"\n",
    "    embeddings = []\n",
    "    logger.info(\"Encoding documents\")\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding documents\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            batch_embeddings = model.encode(\n",
    "                batch,\n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=batch_size,\n",
    "                normalize_embeddings=True,\n",
    "                max_seq_length=512\n",
    "            )\n",
    "            embeddings.append(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error on batch {i // batch_size}: {e}\")\n",
    "            embeddings.append(np.zeros((len(batch), model.get_sentence_embedding_dimension())))\n",
    "    if not embeddings:\n",
    "        raise ValueError(\"No document embeddings generated.\")\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "try:\n",
    "    doc_embeddings = encode_documents(texts)\n",
    "    logger.info(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to encode documents: {e}\")\n",
    "    print(f\"Error: Failed to encode documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# Part 6: Encode Query\n",
    "def encode_query(text: str) -> np.ndarray:\n",
    "    \"\"\"Encode a single query.\"\"\"\n",
    "    text = preprocess_text(text)\n",
    "    text = expand_query(text)\n",
    "    try:\n",
    "        embedding = model.encode(\n",
    "            [text],\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            normalize_embeddings=True,\n",
    "            max_seq_length=512\n",
    "        )\n",
    "        return embedding[0]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to encode query: {e}\")\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "# Part 7: Retrieval Function\n",
    "def retrieve(query: str, k: int = 10, alpha: float = 0.7) -> List[tuple]:\n",
    "    \"\"\"Retrieve top-k documents using hybrid scoring and reranking.\"\"\"\n",
    "    query_clean = preprocess_text(query)\n",
    "    query_vec = encode_query(query_clean)\n",
    "    try:\n",
    "        similarities = cosine_similarity([query_vec], doc_embeddings)[0]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to compute cosine similarity: {e}\")\n",
    "        similarities = np.zeros(len(doc_embeddings))\n",
    "\n",
    "    tokenized_query = query_clean.split()\n",
    "    try:\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)\n",
    "        bm25_scores = bm25_scores / (np.max(bm25_scores) + 1e-10)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to compute BM25 scores: {e}\")\n",
    "        bm25_scores = np.zeros(len(doc_embeddings))\n",
    "\n",
    "    combined_scores = alpha * similarities + (1 - alpha) * bm25_scores\n",
    "    top_k_idx = combined_scores.argsort()[-k:][::-1]\n",
    "    top_k_cases = [case_ids[i] for i in top_k_idx]\n",
    "    pairs = [[query_clean, texts[i]] for i in top_k_idx]\n",
    "\n",
    "    try:\n",
    "        rerank_scores = cross_encoder.predict(pairs)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to rerank: {e}\")\n",
    "        rerank_scores = np.zeros(len(pairs))\n",
    "\n",
    "    reranked_idx = np.argsort(rerank_scores)[::-1][:min(5, len(rerank_scores))]\n",
    "    results = [(top_k_cases[i], float(rerank_scores[i])) for i in reranked_idx]\n",
    "    return results\n",
    "\n",
    "# Part 8: Evaluation Metrics\n",
    "def calculate_mrr(results, true_id):\n",
    "    \"\"\"Calculate Mean Reciprocal Rank.\"\"\"\n",
    "    for rank, (case_id, _) in enumerate(results, 1):\n",
    "        if case_id == true_id:\n",
    "            return 1 / rank\n",
    "    return 0\n",
    "\n",
    "# Part 9: Load Evaluation Queries\n",
    "try:\n",
    "    logger.info(f\"Loading evaluation queries from {PATH_QUERIES}\")\n",
    "    with open(PATH_QUERIES, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_queries = json.load(f)\n",
    "    logger.info(f\"Loaded {len(eval_queries)} evaluation queries\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"File {PATH_QUERIES} not found. Please ensure queries.json exists.\")\n",
    "    print(f\"Error: File {PATH_QUERIES} not found.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading {PATH_QUERIES}: {e}\")\n",
    "    print(f\"Error loading {PATH_QUERIES}: {e}\")\n",
    "    raise\n",
    "\n",
    "# Part 10: Hyperparameter Tuning\n",
    "alpha_values = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "best_alpha = 0.7\n",
    "best_accuracy = 0\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    logger.info(f\"Evaluating with alpha={alpha}\")\n",
    "    correct_5 = 0\n",
    "    correct_10 = 0\n",
    "    mrr_sum = 0\n",
    "    for item in eval_queries:\n",
    "        query = item[\"query\"]\n",
    "        true_id = item[\"case_id\"]\n",
    "        try:\n",
    "            results = retrieve(query, k=10, alpha=alpha)\n",
    "            top_5 = results[:5]\n",
    "            if any(true_id == result[0] for result in top_5):\n",
    "                correct_5 += 1\n",
    "            if any(true_id == result[0] for result in results):\n",
    "                correct_10 += 1\n",
    "            mrr_sum += calculate_mrr(results, true_id)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating query '{query[:50]}...': {e}\")\n",
    "            continue\n",
    "\n",
    "    accuracy_5 = correct_5 / len(eval_queries)\n",
    "    logger.info(f\"Alpha={alpha}: Accuracy@5={accuracy_5:.2f}, Recall@10={correct_10 / len(eval_queries):.2f}, MRR={mrr_sum / len(eval_queries):.2f}\")\n",
    "    if accuracy_5 > best_accuracy:\n",
    "        best_accuracy = accuracy_5\n",
    "        best_alpha = alpha\n",
    "\n",
    "logger.info(f\"Best alpha: {best_alpha} with Accuracy@5: {best_accuracy:.2f}\")\n",
    "print(f\"Best alpha: {best_alpha} with Accuracy@5: {best_accuracy:.2f}\")\n",
    "\n",
    "# Part 11: Final Evaluation with Best Alpha\n",
    "correct_5 = 0\n",
    "correct_10 = 0\n",
    "mrr_sum = 0\n",
    "for item in tqdm(eval_queries, desc=\"Evaluating queries\"):\n",
    "    query = item[\"query\"]\n",
    "    true_id = item[\"case_id\"]\n",
    "    try:\n",
    "        results = retrieve(query, k=10, alpha=best_alpha)\n",
    "        top_5 = results[:5]\n",
    "        logger.info(f\"Query: {query[:100]}...\")\n",
    "        logger.info(f\"Top 5 Results (ID, Score): {top_5}\")\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"Top 5 Results (ID, Score): {top_5}\")\n",
    "\n",
    "        if any(true_id == result[0] for result in top_5):\n",
    "            logger.info(f\"✅ FOUND in Top-5: {true_id}\")\n",
    "            print(f\"✅ FOUND in Top-5: {true_id}\")\n",
    "            correct_5 += 1\n",
    "        else:\n",
    "            logger.info(f\"❌ NOT FOUND in Top-5: {true_id}\")\n",
    "            print(f\"❌ NOT FOUND in Top-5: {true_id}\")\n",
    "            try:\n",
    "                true_idx = case_ids.index(true_id)\n",
    "                true_text = texts[true_idx][:300] + \"...\" if len(texts[true_idx]) > 300 else texts[true_idx]\n",
    "                logger.info(f\"True Case Text: {true_text}\")\n",
    "                print(f\"True Case Text: {true_text}\")\n",
    "                query_words = set(preprocess_text(query).split())\n",
    "                true_words = set(true_text.split())\n",
    "                common_words = query_words.intersection(true_words)\n",
    "                logger.info(f\"Common Words: {common_words}\")\n",
    "                print(f\"Common Words: {common_words}\")\n",
    "                query_vec = encode_query(query)\n",
    "                true_vec = doc_embeddings[true_idx]\n",
    "                cos_sim = cosine_similarity([query_vec], [true_vec])[0][0]\n",
    "                logger.info(f\"Cosine Similarity with True Doc: {cos_sim:.4f}\")\n",
    "                print(f\"Cosine Similarity with True Doc: {cos_sim:.4f}\")\n",
    "            except ValueError:\n",
    "                logger.warning(f\"True case ID {true_id} not found in dataset.\")\n",
    "                print(f\"Warning: True case ID {true_id} not found in dataset.\")\n",
    "\n",
    "        if any(true_id == result[0] for result in results):\n",
    "            correct_10 += 1\n",
    "        mrr_sum += calculate_mrr(results, true_id)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error evaluating query '{query[:50]}...': {e}\")\n",
    "        print(f\"Error evaluating query: {e}\")\n",
    "        continue\n",
    "\n",
    "# Part 12: Print Final Results\n",
    "logger.info(f\"IndoBERT-Base-P1 Accuracy@5: {correct_5}/{len(eval_queries)} = {correct_5 / len(eval_queries):.2f}\")\n",
    "logger.info(f\"IndoBERT-Base-P1 Recall@10: {correct_10}/{len(eval_queries)} = {correct_10 / len(eval_queries):.2f}\")\n",
    "logger.info(f\"IndoBERT-Base-P1 MRR: {mrr_sum / len(eval_queries):.2f}\")\n",
    "print(f\"\\nIndoBERT-Base-P1 Accuracy@5: {correct_5}/{len(eval_queries)} = {correct_5 / len(eval_queries):.2f}\")\n",
    "print(f\"IndoBERT-Base-P1 Recall@10: {correct_10}/{len(eval_queries)} = {correct_10 / len(eval_queries):.2f}\")\n",
    "print(f\"IndoBERT-Base-P1 MRR: {mrr_sum / len(eval_queries):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f823a",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "670c2929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 00:22:14,516 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\logs\n",
      "2025-06-27 00:22:14,518 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\processed\n",
      "2025-06-27 00:22:14,520 - INFO - Directory ensured: d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\eval\n",
      "2025-06-27 00:22:14,522 - INFO - Starting retrieval process at 2025-06-27 00:22:14\n",
      "2025-06-27 00:22:14,526 - INFO - Loaded 50 cases from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\processed\\cases.csv\n",
      "2025-06-27 00:22:14,533 - INFO - TF-IDF matrix shape: (50, 267)\n",
      "2025-06-27 00:22:14,536 - INFO - Loaded 5 evaluation queries from d:\\TUGAS SMT 6\\PENALARAN KOMPUTER\\TUGAS AKHIR UAS\\CODE\\Penalaran_Komputer\\CBR_Penalararan_Komputer(8)\\data\\eval\\queries.json\n",
      "2025-06-27 00:22:14,592 - INFO - Training data shape: (100, 537)\n",
      "2025-06-27 00:22:14,594 - INFO - Class distribution: [50 50]\n",
      "2025-06-27 00:22:14,731 - INFO - Best Logistic Regression Parameters: {'C': 0.01}\n",
      "2025-06-27 00:22:14,732 - ERROR - Error training Logistic Regression: 'GridSearchCV' object has no attribute 'best_score'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (100, 537)\n",
      "Class distribution: [50 50]\n",
      "Error training Logistic Regression: 'GridSearchCV' object has no attribute 'best_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 00:22:14,797 - INFO - Query: Legal dispute case 1...\n",
      "2025-06-27 00:22:14,799 - INFO - Top 5 Results (ID, Score): [('CASE_1', np.float64(0.6076643290171767)), ('CASE_2', np.float64(0.5945989349663527)), ('CASE_3', np.float64(0.5945989349663527)), ('CASE_4', np.float64(0.5945989349663527)), ('CASE_5', np.float64(0.5945989349663527))]\n",
      "2025-06-27 00:22:14,799 - INFO - ✅ FOUND in Top-5: CASE_1\n",
      "2025-06-27 00:22:14,884 - INFO - Query: Legal dispute case 2...\n",
      "2025-06-27 00:22:14,886 - INFO - Top 5 Results (ID, Score): [('CASE_2', np.float64(0.6076643290171767)), ('CASE_1', np.float64(0.5945989349663527)), ('CASE_3', np.float64(0.5945989349663527)), ('CASE_4', np.float64(0.5945989349663527)), ('CASE_5', np.float64(0.5945989349663527))]\n",
      "2025-06-27 00:22:14,888 - INFO - ✅ FOUND in Top-5: CASE_2\n",
      "2025-06-27 00:22:14,965 - INFO - Query: Legal dispute case 3...\n",
      "2025-06-27 00:22:14,967 - INFO - Top 5 Results (ID, Score): [('CASE_3', np.float64(0.6076643290171767)), ('CASE_1', np.float64(0.5945989349663527)), ('CASE_2', np.float64(0.5945989349663527)), ('CASE_4', np.float64(0.5945989349663527)), ('CASE_5', np.float64(0.5945989349663527))]\n",
      "2025-06-27 00:22:14,968 - INFO - ✅ FOUND in Top-5: CASE_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Legal dispute case 1\n",
      "Top 5 Results (ID, Score): [('CASE_1', np.float64(0.6076643290171767)), ('CASE_2', np.float64(0.5945989349663527)), ('CASE_3', np.float64(0.5945989349663527)), ('CASE_4', np.float64(0.5945989349663527)), ('CASE_5', np.float64(0.5945989349663527))]\n",
      "✅ FOUND in Top-5: CASE_1\n",
      "\n",
      "Query: Legal dispute case 2\n",
      "Top 5 Results (ID, Score): [('CASE_2', np.float64(0.6076643290171767)), ('CASE_1', np.float64(0.5945989349663527)), ('CASE_3', np.float64(0.5945989349663527)), ('CASE_4', np.float64(0.5945989349663527)), ('CASE_5', np.float64(0.5945989349663527))]\n",
      "✅ FOUND in Top-5: CASE_2\n",
      "\n",
      "Query: Legal dispute case 3\n",
      "Top 5 Results (ID, Score): [('CASE_3', np.float64(0.6076643290171767)), ('CASE_1', np.float64(0.5945989349663527)), ('CASE_2', np.float64(0.5945989349663527)), ('CASE_4', np.float64(0.5945989349663527)), ('CASE_5', np.float64(0.5945989349663527))]\n",
      "✅ FOUND in Top-5: CASE_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 00:22:15,033 - INFO - Query: Legal dispute case 4...\n",
      "2025-06-27 00:22:15,034 - INFO - Top 5 Results (ID, Score): [('CASE_4', np.float64(0.6076643290171767)), ('CASE_1', np.float64(0.5945989349663527)), ('CASE_2', np.float64(0.5945989349663527)), ('CASE_3', np.float64(0.5945989349663527)), ('CASE_5', np.float64(0.5945989349663527))]\n",
      "2025-06-27 00:22:15,035 - INFO - ✅ FOUND in Top-5: CASE_4\n",
      "2025-06-27 00:22:15,096 - INFO - Query: Legal dispute case 5...\n",
      "2025-06-27 00:22:15,097 - INFO - Top 5 Results (ID, Score): [('CASE_5', np.float64(0.6076643290171767)), ('CASE_1', np.float64(0.5945989349663527)), ('CASE_2', np.float64(0.5945989349663527)), ('CASE_3', np.float64(0.5945989349663527)), ('CASE_4', np.float64(0.5945989349663527))]\n",
      "2025-06-27 00:22:15,098 - INFO - ✅ FOUND in Top-5: CASE_5\n",
      "2025-06-27 00:22:15,098 - INFO - LogReg Pairwise Accuracy@5: 5/5 = 1.00\n",
      "2025-06-27 00:22:15,100 - INFO - LogReg Pairwise Recall@10: 5/5 = 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Legal dispute case 4\n",
      "Top 5 Results (ID, Score): [('CASE_4', np.float64(0.6076643290171767)), ('CASE_1', np.float64(0.5945989349663527)), ('CASE_2', np.float64(0.5945989349663527)), ('CASE_3', np.float64(0.5945989349663527)), ('CASE_5', np.float64(0.5945989349663527))]\n",
      "✅ FOUND in Top-5: CASE_4\n",
      "\n",
      "Query: Legal dispute case 5\n",
      "Top 5 Results (ID, Score): [('CASE_5', np.float64(0.6076643290171767)), ('CASE_1', np.float64(0.5945989349663527)), ('CASE_2', np.float64(0.5945989349663527)), ('CASE_3', np.float64(0.5945989349663527)), ('CASE_4', np.float64(0.5945989349663527))]\n",
      "✅ FOUND in Top-5: CASE_5\n",
      "\n",
      "LogReg Pairwise Accuracy@5: 5/5 = 1.00\n",
      "LogReg Pairwise Recall@10: 5/5 = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Import Libraries and Initialize\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define base directory (aligned with previous scripts)\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.getcwd())  # Parent of 'notebooks'\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # Fallback for interactive environments like Jupyter\n",
    "\n",
    "# Define paths\n",
    "PATH_CSV = os.path.join(BASE_DIR, 'data', 'processed', 'cases.csv')\n",
    "PATH_QUERIES = os.path.join(BASE_DIR, 'data', 'eval', 'queries.json')\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "LOG_PATH = os.path.join(LOG_DIR, 'retrieval_logreg.log')\n",
    "\n",
    "# Validate path length for Windows\n",
    "MAX_PATH_LENGTH = 260\n",
    "\n",
    "def validate_path(path):\n",
    "    if len(path) > MAX_PATH_LENGTH:\n",
    "        raise ValueError(f\"Path {path} exceeds Windows maximum length of {MAX_PATH_LENGTH} characters\")\n",
    "    return path\n",
    "\n",
    "# Ensure directories exist\n",
    "for path in [LOG_DIR, os.path.dirname(PATH_CSV), os.path.dirname(PATH_QUERIES)]:\n",
    "    try:\n",
    "        validate_path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logging.info(f\"Directory ensured: {path}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Path validation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH, mode='w', encoding='utf-8'),  # Overwrite mode\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logging.info(\"Starting retrieval process at %s\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Part 2: Text Preprocessing\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocess text by lowercasing, removing punctuation, and normalizing spaces.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Part 3: Feature Extraction\n",
    "def extract_features(query_vec, doc_vec, query_text: str, doc_text: str) -> np.ndarray:\n",
    "    \"\"\"Extract features including TF-IDF vectors, cosine similarity, word overlap, and coverage.\"\"\"\n",
    "    query_vec = query_vec.toarray()[0]\n",
    "    doc_vec = doc_vec.toarray()[0]\n",
    "    cos_sim = cosine_similarity([query_vec], [doc_vec])[0][0]\n",
    "    query_words = set(query_text.split())\n",
    "    doc_words = set(doc_text.split())\n",
    "    overlap = len(query_words.intersection(doc_words)) / max(len(query_words), 1)\n",
    "    coverage = len(query_words.intersection(doc_words)) / max(len(query_words), 1)\n",
    "    combined_vec = np.concatenate([query_vec, doc_vec, [cos_sim, overlap, coverage]])\n",
    "    return combined_vec\n",
    "\n",
    "# Part 4: Load and Prepare Data\n",
    "try:\n",
    "    df = pd.read_csv(PATH_CSV)\n",
    "    logging.info(f\"Loaded {len(df)} cases from {PATH_CSV}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"File {PATH_CSV} not found. Please ensure 02_representation.py has run successfully.\")\n",
    "    print(f\"Error: File {PATH_CSV} not found.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading {PATH_CSV}: {e}\")\n",
    "    print(f\"Error loading {PATH_CSV}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "texts = df[\"ringkasan_fakta\"].fillna(\"\").apply(preprocess_text)\n",
    "case_ids = df[\"case_id\"].tolist()\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "id_stop_words = [\n",
    "    \"dan\", \"di\", \"dari\", \"ke\", \"pada\", \"dengan\", \"untuk\", \"yang\", \"ini\", \"itu\",\n",
    "    \"adalah\", \"tersebut\", \"sebagai\", \"oleh\", \"atau\", \"tetapi\", \"karena\", \"jika\",\n",
    "    \"dalam\", \"bagi\", \"tentang\", \"melalui\", \"serta\", \"maka\", \"lagi\", \"sudah\",\n",
    "    \"belum\", \"hanya\", \"saja\", \"bahwa\", \"apa\", \"siapa\", \"bagaimana\", \"kapan\",\n",
    "    \"dimana\", \"kenapa\", \"sejak\", \"hingga\", \"agar\", \"supaya\", \"meskipun\", \"walau\",\n",
    "    \"kecuali\", \"terhadap\", \"antara\", \"selain\", \"setiap\", \"sebelum\", \"sesudah\"\n",
    "]\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=4000,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=id_stop_words\n",
    ")\n",
    "try:\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    logging.info(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error in TF-IDF vectorization: {e}\")\n",
    "    print(f\"Error in TF-IDF vectorization: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load evaluation queries\n",
    "try:\n",
    "    with open(PATH_QUERIES, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_queries = json.load(f)\n",
    "    logging.info(f\"Loaded {len(eval_queries)} evaluation queries from {PATH_QUERIES}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"File {PATH_QUERIES} not found. Please ensure queries.json exists.\")\n",
    "    print(f\"Error: File {PATH_QUERIES} not found.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading {PATH_QUERIES}: {e}\")\n",
    "    print(f\"Error loading {PATH_QUERIES}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Part 5: Prepare Training Data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for item in eval_queries:\n",
    "    query = preprocess_text(item[\"query\"])\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    try:\n",
    "        true_id = item[\"case_id\"]\n",
    "        true_idx = case_ids.index(true_id)\n",
    "    except ValueError:\n",
    "        logging.warning(f\"Case ID {true_id} not found in dataset. Skipping query: {item['query'][:50]}...\")\n",
    "        continue\n",
    "\n",
    "    true_vec = tfidf_matrix[true_idx]\n",
    "    pos_features = extract_features(query_vec, true_vec, query, texts[true_idx])\n",
    "\n",
    "    neg_indices = [i for i in range(len(case_ids)) if i != true_idx]\n",
    "    neg_samples = np.random.choice(neg_indices, size=min(10, len(neg_indices)), replace=False)\n",
    "\n",
    "    for neg_idx in neg_samples:\n",
    "        neg_vec = tfidf_matrix[neg_idx]\n",
    "        neg_features = extract_features(query_vec, neg_vec, query, texts[neg_idx])\n",
    "        diff_vec = pos_features - neg_features\n",
    "        X_train.append(diff_vec)\n",
    "        y_train.append(1)\n",
    "\n",
    "        diff_vec_neg = neg_features - pos_features\n",
    "        X_train.append(diff_vec_neg)\n",
    "        y_train.append(0)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "logging.info(f\"Training data shape: {X_train.shape}\")\n",
    "logging.info(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "\n",
    "# Part 6: Train Logistic Regression\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logreg = GridSearchCV(\n",
    "    LogisticRegression(max_iter=5000, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "try:\n",
    "    logreg.fit(X_train, y_train)\n",
    "    logging.info(f\"Best Logistic Regression Parameters: {logreg.best_params_}\")\n",
    "    logging.info(f\"Best CV Accuracy: {logreg.best_score:.2f}\")\n",
    "    print(f\"Best Logistic Regression Parameters: {logreg.best_params_}\")\n",
    "    print(f\"Best CV Accuracy: {logreg.best_score:.2f}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error training Logistic Regression: {e}\")\n",
    "    print(f\"Error training Logistic Regression: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Part 7: Retrieval Function\n",
    "def retrieve(query: str, k: int = 5) -> List[tuple]:\n",
    "    \"\"\"Retrieve top-k cases using Logistic Regression scoring.\"\"\"\n",
    "    try:\n",
    "        query = preprocess_text(query)\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        scores = []\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            doc_vec = tfidf_matrix[i]\n",
    "            features = extract_features(query_vec, doc_vec, query, texts[i])\n",
    "            score = logreg.predict_proba([features])[0][1]  # Probability of positive class\n",
    "            scores.append((case_ids[i], score))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:k]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in retrieval for query '{query[:50]}...': {e}\")\n",
    "        print(f\"Error in retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "# Part 8: Evaluation\n",
    "correct_5 = 0\n",
    "correct_10 = 0\n",
    "for item in eval_queries:\n",
    "    query = item[\"query\"]\n",
    "    true_id = item[\"case_id\"]\n",
    "    try:\n",
    "        results = retrieve(query, k=10)\n",
    "        top_5 = results[:5]\n",
    "        logging.info(f\"Query: {query[:100]}...\")\n",
    "        logging.info(f\"Top 5 Results (ID, Score): {top_5}\")\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"Top 5 Results (ID, Score): {top_5}\")\n",
    "\n",
    "        if any(true_id == result[0] for result in top_5):\n",
    "            logging.info(f\"✅ FOUND in Top-5: {true_id}\")\n",
    "            print(f\"✅ FOUND in Top-5: {true_id}\")\n",
    "            correct_5 += 1\n",
    "        else:\n",
    "            logging.info(f\"❌ NOT FOUND in Top-5: {true_id}\")\n",
    "            print(f\"❌ NOT FOUND in Top-5: {true_id}\")\n",
    "            try:\n",
    "                true_idx = case_ids.index(true_id)\n",
    "                true_text = texts[true_idx][:300] + \"...\" if len(texts[true_idx]) > 300 else texts[true_idx]\n",
    "                logging.info(f\"True Case Text: {true_text}\")\n",
    "                print(f\"True Case Text: {true_text}\")\n",
    "                query_words = set(preprocess_text(query).split())\n",
    "                true_words = set(true_text.split())\n",
    "                common_words = query_words.intersection(true_words)\n",
    "                logging.info(f\"Common Words: {common_words}\")\n",
    "                print(f\"Common Words: {common_words}\")\n",
    "                query_vec = vectorizer.transform([query])\n",
    "                true_vec = tfidf_matrix[true_idx]\n",
    "                cos_sim = cosine_similarity(query_vec, true_vec)[0][0]\n",
    "                logging.info(f\"Cosine Similarity with True Doc: {cos_sim:.4f}\")\n",
    "                print(f\"Cosine Similarity with True Doc: {cos_sim:.4f}\")\n",
    "            except ValueError:\n",
    "                logging.warning(f\"True case ID {true_id} not found in dataset.\")\n",
    "                print(f\"Warning: True case ID {true_id} not found in dataset.\")\n",
    "\n",
    "        if any(true_id == result[0] for result in results):\n",
    "            correct_10 += 1\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error evaluating query '{query[:50]}...': {e}\")\n",
    "        print(f\"Error evaluating query: {e}\")\n",
    "        continue\n",
    "\n",
    "logging.info(f\"LogReg Pairwise Accuracy@5: {correct_5}/{len(eval_queries)} = {correct_5 / len(eval_queries):.2f}\")\n",
    "logging.info(f\"LogReg Pairwise Recall@10: {correct_10}/{len(eval_queries)} = {correct_10 / len(eval_queries):.2f}\")\n",
    "print(f\"\\nLogReg Pairwise Accuracy@5: {correct_5}/{len(eval_queries)} = {correct_5 / len(eval_queries):.2f}\")\n",
    "print(f\"LogReg Pairwise Recall@10: {correct_10}/{len(eval_queries)} = {correct_10 / len(eval_queries):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bba967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
